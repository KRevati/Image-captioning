{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet101_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHFL6Xe6vM1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "\n",
        "import numpy as np \n",
        "import os\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import preprocess_input\n",
        "from PIL import Image\n",
        "import math\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.layers import (GRU, BatchNormalization, Dense, Dropout, Embedding,\n",
        "                          Input, Lambda, TimeDistributed, RepeatVector, concatenate)\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9f_ZFOp5NrHz"
      },
      "source": [
        "# IMAGE PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jpk2N7yYNrHw",
        "colab": {}
      },
      "source": [
        "# extract features of images in flickr8k dataset directory\n",
        "\n",
        "def feature_extraction(directory):\n",
        "    base_model = ResNet101(weights='imagenet')\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "    img_id = []\n",
        "    img_matrices = []\n",
        "    for img_file in os.listdir(directory):\n",
        "        img_path = directory + '/' + img_file\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        x = image.img_to_array(img)\n",
        "        x = preprocess_input(x)\n",
        "        img_id.append(os.path.splitext(img_file)[0])\n",
        "        img_matrices.append(x)    \n",
        "    img_matrices = np.array(img_matrices)\n",
        "    assert(len(img_matrices.shape)==4)\n",
        "    img_features = model.predict(img_matrices, verbose=1)\n",
        "    return {'ids':img_id, 'features':img_features}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_directory = 'Flicker8k_Dataset'\n",
        "    print(\"extracting features...\")\n",
        "    features_dict = feature_extraction(image_directory)\n",
        "    dump(features_dict, open('features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l5tI66edNrHp",
        "colab": {}
      },
      "source": [
        "# load the extracted features\n",
        "\n",
        "def loading_features(dict_dir, dataset_dir, repeat_times = 1):\n",
        "    assert(repeat_times >= 1)\n",
        "    img_ids = []\n",
        "    with open(dataset_dir, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            img_ids.append(os.path.splitext(line)[0])\n",
        "    features_dict = load(open(dict_dir, 'rb'))\n",
        "    dataset_features = []\n",
        "    for img_id in img_ids:\n",
        "        fidx = features_dict['ids'].index(img_id)\n",
        "        dataset_features.append(np.vstack([features_dict['features'][fidx, :]]*repeat_times))\n",
        "    dataset_features = np.vstack(dataset_features)\n",
        "    return dataset_features\n",
        "\n",
        "# extract features from an image \n",
        "\n",
        "def extracting_features_from_image(file_dir):\n",
        "    img = image.load_img(file_dir, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    base_model = ResNet101(weights='imagenet')\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "    return model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bYSbP_lFNrHo"
      },
      "source": [
        "# TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "teHAP6RLNrHm",
        "colab": {}
      },
      "source": [
        "# separate captions from 'token.txt'\n",
        "\n",
        "def load_token_text(token_dir):\n",
        "    sents_dict = {}\n",
        "    with open(token_dir, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            words = line.strip('\\n').split()\n",
        "            img_id = words[0].split('.')[0]\n",
        "            sent = ' '.join(words[1:])\n",
        "            if img_id in sents_dict.keys():\n",
        "                sents_dict[img_id].append(sent)\n",
        "            else:\n",
        "                sents_dict[img_id] = [sent]           \n",
        "    return sents_dict\n",
        "\n",
        "# append captions with 'startseq' and 'endseq'\n",
        "\n",
        "def load_dataset_token(dataset_dir, token_dir, start_end = True):\n",
        "    all_sents = load_token_text(token_dir)\n",
        "    img_ids = []\n",
        "    with open(dataset_dir, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            img_ids.append(os.path.splitext(line)[0])\n",
        "    sent_list = []\n",
        "    for id in img_ids:\n",
        "        for sent in all_sents[id]:\n",
        "            sent_ = sent\n",
        "            if start_end:\n",
        "                sent_ = 'startseq ' + sent_ + ' endseq'\n",
        "            sent_list.append(sent_)    \n",
        "    return sent_list\n",
        "\n",
        "#tokenize the captions\n",
        "\n",
        "def create_tokenizer(dataset_dir, token_dir, start_end = True, use_all = False):\n",
        "    num_words = None\n",
        "    sent_list = load_dataset_token(dataset_dir, token_dir, start_end)\n",
        "    if use_all:\n",
        "        tokenizer = Tokenizer()\n",
        "    else:\n",
        "        if num_words:\n",
        "            tokenizer = Tokenizer(num_words)\n",
        "        else:\n",
        "            tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sent_list)\n",
        "    return tokenizer\n",
        "\n",
        "# fit the tokenizer on captions\n",
        "\n",
        "def clean_test_sentences(tokenizer, sents_list):\n",
        "    cleaned_sents_list= []\n",
        "    for sents in sents_list:\n",
        "        sequences = tokenizer.texts_to_sequences(sents)\n",
        "        cleaned_sents_list.append(tokenizer.sequences_to_texts(sequences))    \n",
        "    return cleaned_sents_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io6ObKXYpt-X",
        "colab_type": "text"
      },
      "source": [
        "#BATCH GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7IpSbYU2Eqjo",
        "colab": {}
      },
      "source": [
        "# generate batches of data for training the model\n",
        "\n",
        "def data_generator(batch_size, max_len, tokenizer, dict_dir, dataset_dir, token_dir):\n",
        "    vocab_size = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "    img_features = loading_features(dict_dir, dataset_dir, 5)\n",
        "    raw_sentences = load_dataset_token(dataset_dir, token_dir, True)\n",
        "    N = img_features.shape[0]    \n",
        "    while True:\n",
        "        for i in range(0, N, batch_size):\n",
        "            sequences = tokenizer.texts_to_sequences(raw_sentences[i:i+batch_size])    \n",
        "            X_text = []\n",
        "            Y_text = []\n",
        "            for seq in sequences:\n",
        "                if len(seq) > max_len:\n",
        "                    X_text.append(seq[:max_len])\n",
        "                    Y_text.append(seq[1:max_len+1])\n",
        "                else:\n",
        "                    X_text.append(seq[:len(seq)-1] + [0]*(max_len-len(seq)+1))\n",
        "                    Y_text.append(seq[1:] + [0]*(max_len-len(seq)+1))\n",
        "            X_text_mat = np.array(X_text)\n",
        "            Y_text_mat = to_categorical(Y_text, vocab_size)\n",
        "            yield ([img_features[i:i+batch_size, :], X_text_mat, np.zeros([X_text_mat.shape[0], unit_size])], \n",
        "                    Y_text_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W0jqDl8-NrHl"
      },
      "source": [
        "# DEFINE PAR-INJECT CONCATENATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S4pPxFNxSoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_size = 512\n",
        "# define the par-inject concat model\n",
        "\n",
        "def par_concat_model(vocab_size, max_len, reg):\n",
        "\n",
        "    # Image embedding\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    X_img = Dropout(0.5)(inputs1)\n",
        "    X_img = Dense(unit_size, use_bias = False, \n",
        "                        kernel_regularizer=regularizers.l2(reg),\n",
        "                        name = 'dense_img')(X_img)\n",
        "    X_img = BatchNormalization(name='batch_normalization_img')(X_img)\n",
        "    X_img = RepeatVector(max_len)(X_img)\n",
        "\n",
        "    # Text embedding\n",
        "    inputs2 = Input(shape=(max_len,))\n",
        "    X_text = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')(inputs2)\n",
        "    X_text = Dropout(0.5)(X_text)\n",
        "\n",
        "    # Initial States\n",
        "    a0 = Input(shape=(unit_size,))\n",
        "    #c0 = Input(shape=(unit_size,))\n",
        "    merge=concatenate([X_img, X_text ])\n",
        "\n",
        "    LSTMLayer = LSTM(unit_size, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\n",
        "    A, a = LSTMLayer(merge, initial_state=[a0])\n",
        "    output = TimeDistributed(Dense(vocab_size, activation='softmax',\n",
        "                                     kernel_regularizer = regularizers.l2(reg), \n",
        "                                     bias_regularizer = regularizers.l2(reg)), name = 'time_distributed_softmax')(A)\n",
        "    return Model(inputs=[inputs1, inputs2, a0], outputs=output, name='par')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjxPqUKNYU-",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING PHASE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSIgLVbixTLD",
        "colab": {}
      },
      "source": [
        "# function for training the model\n",
        "\n",
        "def training(dirs_dict, lr, decay, reg, batch_size, epochs, max_len, initial_epoch, previous_model = None):\n",
        "    dict_dir = dirs_dict['dict_dir']\n",
        "    token_dir = dirs_dict['token_dir']\n",
        "    train_dir = dirs_dict['train_dir']\n",
        "    dev_dir = dirs_dict['dev_dir']\n",
        "    params_dir = dirs_dict['params_dir']\n",
        "\n",
        "    # Use Tokenizer to create vocabulary\n",
        "    tokenizer = create_tokenizer(train_dir, token_dir, start_end = True)\n",
        "    \n",
        "    # loading data\n",
        "    generator_train = data_generator(batch_size, max_len, tokenizer, dict_dir, train_dir, token_dir)\n",
        "    generator_dev = data_generator(50, max_len, tokenizer, dict_dir, dev_dir, token_dir)\n",
        "\n",
        "    vocab_size = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "\n",
        "    # Define model \n",
        "    par_model = par_concat_model(vocab_size, max_len, reg)\n",
        "\n",
        "    if not previous_model:\n",
        "        par_model.summary()\n",
        "        plot_model(par_model, to_file='model.png',show_shapes=True)\n",
        "    else:\n",
        "        par_model.load_weights(previous_model, by_name = True, skip_mismatch=True)\n",
        "\n",
        "    # Define checkpoint callback\n",
        "    file_path = params_dir + '/model-ep{epoch:03d}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5'\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_weights_only = True, period=1)\n",
        "    EarlyStop=EarlyStopping(monitor='val_loss',mode='min', patience=5)\n",
        "\n",
        "    # Compile the model\n",
        "    par_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
        "\n",
        "    # training\n",
        "    history=par_model.fit_generator(generator_train, steps_per_epoch=30000//batch_size, epochs=epochs, verbose=1, \n",
        "                            callbacks=[checkpoint,EarlyStop],\n",
        "                            validation_data = generator_dev, validation_steps = 100, initial_epoch = initial_epoch)\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    dict_dir = 'features.pkl'\n",
        "    train_dir = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "    dev_dir = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "    token_dir = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "    # folder to save model weights\n",
        "    params_dir = 'RESNET101/MODELS'\n",
        "\n",
        "    dirs_dict={'dict_dir':dict_dir, 'train_dir':train_dir, 'dev_dir':dev_dir, \n",
        "                'token_dir':token_dir, 'params_dir':params_dir}   \n",
        "    training(dirs_dict, lr=0.01, decay=1e-6, reg = 1e-4, batch_size = 240, epochs = 2000, \n",
        "             max_len = 24, initial_epoch = 0, previous_model = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M14sTh_LNrHQ"
      },
      "source": [
        "# DECODING PHASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFSU-1GZZYum",
        "colab_type": "text"
      },
      "source": [
        "# Greedy search decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcU-jtEhYK9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the greedy model\n",
        "\n",
        "def greedy_model(vocab_size, max_len):    \n",
        "    EncoderDense = Dense(unit_size, use_bias=False, name = 'dense_img')\n",
        "    EmbeddingLayer = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')\n",
        "    LSTMLayer = GRU(unit_size, return_state = True, name = 'lstm')\n",
        "    SoftmaxLayer = Dense(vocab_size, activation='softmax', name = 'time_distributed_softmax')\n",
        "    BatchNormLayer = BatchNormalization(name='batch_normalization_img')\n",
        "\n",
        "    # Image embedding\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    X_img = EncoderDense(inputs1)\n",
        "    X_img = BatchNormLayer(X_img)\n",
        "    X_img = RepeatVector(1)(X_img)\n",
        "\n",
        "    # Text embedding\n",
        "    inputs2 = Input(shape=(1,))\n",
        "    X_text = EmbeddingLayer(inputs2)\n",
        "\n",
        "    # Initial States\n",
        "    a0 = Input(shape=(unit_size,))\n",
        "    #c0 = Input(shape=(unit_size,))\n",
        "\n",
        "    a=a0;\n",
        "    outputs = []\n",
        "    for i in range(max_len):\n",
        "        merge=concatenate([X_img,X_text])\n",
        "        A, a = LSTMLayer(merge,initial_state=[a])\n",
        "        output = SoftmaxLayer(A)\n",
        "        outputs.append(output)\n",
        "        x = Lambda(lambda x : K.expand_dims(K.argmax(x)))(output)\n",
        "        X_text= EmbeddingLayer(x)   \n",
        "    return Model(inputs=[inputs1, inputs2, a0], outputs=outputs, name='par_greedy_inference_v2')\n",
        "\n",
        "# predict words from dictionary using greedy search\n",
        "\n",
        "def decoder_greedy(inf_model, tokenizer, features, post_process = True):\n",
        "    assert(features.shape[0]>0 and features.shape[1] == 2048)\n",
        "    N = features.shape[0]\n",
        "    startseq = np.repeat([tokenizer.word_index['startseq']], N)\n",
        "    a0 = np.zeros([N, unit_size])\n",
        "    #c0 = np.zeros([N, unit_size])\n",
        "    y_preds = np.array(inf_model.predict([features, startseq, a0], verbose = 1))\n",
        "    y_preds = np.transpose(y_preds, axes = [1,0,2])   \n",
        "    sequences = np.argmax(y_preds, axis = -1)\n",
        "    sents = tokenizer.sequences_to_texts(sequences)\n",
        "    if post_process:\n",
        "        # post processing: 'endseq'\n",
        "        sents_pp = []\n",
        "        for sent in sents:\n",
        "            if 'endseq' in sent.split():\n",
        "                words = sent.split()\n",
        "                sents_pp.append(' '.join(words[:words.index('endseq')]))\n",
        "            else:\n",
        "                sents_pp.append(sent)\n",
        "        sents = sents_pp\n",
        "    return sents\n",
        "\n",
        "# generate captions using greedy search\n",
        "\n",
        "def generate_captions_greedy(model_dir, tokenizer, test_references, test_features, max_len):\n",
        "    vocab_size = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "\n",
        "    # prepare inference model\n",
        "    par_inference = greedy_model(vocab_size, max_len)\n",
        "    par_inference.load_weights(model_dir, by_name = True, skip_mismatch=True)\n",
        "    test_candidates = decoder_greedy(par_inference, tokenizer, test_features, True)\n",
        "    assert(len(test_references) == len(test_candidates))\n",
        "    for i in range(len(test_candidates)):\n",
        "        references = [r.lower().split() for r in test_references[i]]\n",
        "        candidate = test_candidates[i].split()\n",
        "    return test_candidates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1lef3cbZwgL",
        "colab_type": "text"
      },
      "source": [
        "# Beam search decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKGaxWRYEiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define beam search model\n",
        "\n",
        "def beamsearch_model(vocab_size):\n",
        "    EmbeddingLayer = Embedding(vocab_size, unit_size, mask_zero = True, name='emb_text')\n",
        "    LSTMLayer = GRU(unit_size, return_state = True, name='lstm')\n",
        "    SoftmaxLayer = Dense(vocab_size, activation='softmax', name='time_distributed_softmax')\n",
        "    EncoderDense = Dense(unit_size, use_bias = False, name = 'dense_img')\n",
        "    BatchNormLayer = BatchNormalization(name = 'batch_normalization_img')\n",
        "    \n",
        "    #image emdedding\n",
        "    inputs = Input(shape=(2048,))\n",
        "    X_img = EncoderDense(inputs)\n",
        "    X_img = BatchNormLayer(X_img)\n",
        "    X_img = RepeatVector(1)(X_img)\n",
        "    \n",
        "    #Text embedding\n",
        "    cur_word = Input(shape=(1,))\n",
        "    X_text = EmbeddingLayer(cur_word)\n",
        "\n",
        "    # initial states\n",
        "    a0 = Input(shape=(unit_size,))\n",
        "    #c0 = Input(shape=(unit_size,))\n",
        "\n",
        "    merge=concatenate([X_img,X_text])\n",
        "    A, a = LSTMLayer(merge,initial_state=[a0])\n",
        "    output = SoftmaxLayer(A)\n",
        "    return Model(inputs=[inputs, cur_word, a0], outputs=[output,a])\n",
        "\n",
        "#search for words in dictionary using beam search\n",
        "\n",
        "def beam_searching(decoder_model, features, a0 , tokenizer, beam_width, max_len):   \n",
        "    assert(a0.shape == (1, unit_size) and isinstance(beam_width, int) and\n",
        "             beam_width > 0 and max_len > 0)\n",
        "\n",
        "    # === first step ====\n",
        "    start_word = np.array([tokenizer.word_index['startseq']])\n",
        "    output, a = decoder_model.predict([features,start_word,  a0], verbose=0)\n",
        "    assert(len(output.shape)==2 and beam_width<=output.shape[1])\n",
        "\n",
        "    # === define data structure and initialization====\n",
        "    \n",
        "    seeds = np.argpartition(-output, beam_width, axis=-1)[0, :beam_width]\n",
        "    start_words = np.array(seeds)\n",
        "    next_activates = np.repeat(a, beam_width, axis = 0)\n",
        "    #next_cells = np.repeat(c, beam_width, axis = 0)\n",
        "    scores = [math.log(output[0, i]) for i in seeds]\n",
        "    routes = [[i] for i in seeds]\n",
        "    res = {'scores':[], 'routes':[]}\n",
        "\n",
        "    # === search ====\n",
        "    for i in range(max_len-1):\n",
        "        next_features = np.repeat(features,next_activates.shape[0], axis = 0)\n",
        "        outputs, activations = decoder_model.predict([next_features, start_words,next_activates], \n",
        "                                                            verbose=0)\n",
        "        next_features=features\n",
        "        # pick <beam_width> highest scores from every route as a candidate\n",
        "        candidates = np.argpartition(-outputs, beam_width, axis=-1)[:,:beam_width]\n",
        "        # r <----> i-th in scores and routes, c is the index of vocabulary\n",
        "        candidates = [(r, c) for r in range(candidates.shape[0]) for c in candidates[r,:]]\n",
        "        # calculate score according to the candidates\n",
        "        candidates_scores = np.array([scores[r] + math.log(outputs[r, c]) for r, c in candidates])\n",
        "        # consider the length of the current sentence\n",
        "        #weigthed_scores = 1/(i+1)**alpha * candidates_scores\n",
        "        if beam_width < len(candidates):\n",
        "            choosen_candidates = np.argpartition(-candidates_scores, beam_width)[:beam_width]\n",
        "        else:\n",
        "            choosen_candidates = np.arange(0, len(candidates))\n",
        "\n",
        "        # update scores, routes\n",
        "        # construct new start_words, activations, cells\n",
        "        start_words = []\n",
        "        next_activates = []\n",
        "        #next_cells = []\n",
        "        updated_scores = []\n",
        "        updated_routes = []\n",
        "        for idx in choosen_candidates:\n",
        "            r, c = candidates[idx]\n",
        "            if c == tokenizer.word_index['endseq']:\n",
        "                res['routes'].append(routes[r])                \n",
        "                if i != 0:\n",
        "                    res['scores'].append(1/len(routes[r])**0.7 * candidates_scores[idx])\n",
        "                else:\n",
        "                    res['scores'].append(-math.inf)               \n",
        "                beam_width -= 1\n",
        "            else:\n",
        "                start_words.append(c)\n",
        "                next_activates.append(activations[r, :])\n",
        "                #next_cells.append(cells[r, :])\n",
        "                updated_scores.append(candidates_scores[idx])\n",
        "                updated_routes.append(routes[r]+[c])\n",
        "\n",
        "        start_words = np.array(start_words)\n",
        "        next_activates = np.array(next_activates)\n",
        "        #next_cells = np.array(next_cells)\n",
        "        scores = updated_scores\n",
        "        routes = updated_routes\n",
        "        if beam_width <= 0:\n",
        "            break\n",
        "    res['scores'] += [1/len(routes[i])**0.7 * scores[i] for i in range(len(scores))]\n",
        "    res['routes'] += routes\n",
        "    return res\n",
        "\n",
        "# generate captions using beam search\n",
        "\n",
        "def generate_captions_beamsearch(model_dir, tokenizer, test_references, test_features, max_len, beam_width):\n",
        "    vocab_size = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "\n",
        "    # prepare inference model\n",
        "    beamsearching_model = beamsearch_model(vocab_size)\n",
        "    beamsearching_model.load_weights(model_dir, by_name = True, skip_mismatch=True)\n",
        "    feature_size = test_features.shape[0]\n",
        "    a0=  np.zeros([feature_size, unit_size])\n",
        "\n",
        "    # generate candidate sentences\n",
        "    test_candidates = []\n",
        "    for i in range(feature_size):\n",
        "        res = beam_searching(beamsearching_model, test_features[i, :].reshape(1,-1), a0[i, :].reshape(1,-1), tokenizer, beam_width, max_len)\n",
        "        best_idx = np.argmax(res['scores'])\n",
        "        test_candidates.append(tokenizer.sequences_to_texts([res['routes'][best_idx]])[0])\n",
        "    assert(len(test_references) == len(test_candidates))\n",
        "    for i in range(len(test_candidates)):\n",
        "        references = [r.split() for r in test_references[i]]\n",
        "        candidate = test_candidates[i].split()\n",
        "    return test_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYXcDM6gaip8",
        "colab_type": "text"
      },
      "source": [
        "SAVING THE CANDIDATE AND REFERENCE CAPTIONS FOR TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IhKQOJA_NrHO",
        "colab": {}
      },
      "source": [
        "train_dir = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "token_dir = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "# load vocabulary\n",
        "tokenizer = create_tokenizer(train_dir, token_dir, start_end = True, use_all=True)\n",
        "vocab_size  = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "max_len = 24 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5eYoz6wNNrHK",
        "colab": {}
      },
      "source": [
        "def load_filckr8k_features(dict_dir, dataset_dir): \n",
        "    img_ids = []\n",
        "    with open(dataset_dir, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            img_ids.append(os.path.splitext(line)[0])\n",
        "    features = loading_features(dict_dir, dataset_dir, repeat_times = 1)\n",
        "    return img_ids, features\n",
        "\n",
        "# function for generating captions \n",
        "\n",
        "def generate_captions(model_dir, method='b', beam_width = 20):\n",
        "    dict_dir = 'features.pkl'\n",
        "    train_dir = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "    test_dir = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "    token_dir = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "    max_len = 24\n",
        "    tokenizer = create_tokenizer(train_dir, token_dir)\n",
        "    filter_tokenizer = create_tokenizer(test_dir, token_dir, use_all=True)\n",
        "    test_ids, test_features = load_filckr8k_features(dict_dir, test_dir)\n",
        "    all_sents = load_token_text(token_dir)\n",
        "    test_references = [all_sents[id] for id in test_ids]\n",
        "    test_references = clean_test_sentences(filter_tokenizer, test_references)\n",
        "    if method == 'g':\n",
        "        candidates = generate_captions_greedy(model_dir, tokenizer, test_references, test_features, max_len)\n",
        "    elif method == 'b':\n",
        "        candidates = generate_captions_beamsearch(model_dir, tokenizer, test_references, test_features, max_len, beam_width)    \n",
        "    return test_ids, test_references, candidates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p3Ijl67ENrG_",
        "colab": {}
      },
      "source": [
        "# save captions generated using greedy search to a json file\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_dir = 'RESNET101/MODELS/model_gru.h5'\n",
        "    img_ids, test_references, candidates = generate_captions(model_dir, method='g')\n",
        "    res={}\n",
        "    gets={}\n",
        "    for i in range(len(img_ids)):\n",
        "      res[img_ids[i]]=[candidates[i]]\n",
        "      gets[img_ids[i]]=test_references[i]   \n",
        "    import json\n",
        "    with open('res_g.json', 'w') as jsonfile:\n",
        "        json.dump(res, jsonfile)\n",
        "    with open('gets_g.json', 'w') as jsonfile:\n",
        "      json.dump(gets, jsonfile)\n",
        "    print('Captions saved to json file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r5ZndW6tNrHG",
        "colab": {}
      },
      "source": [
        "# save captions generated using beam search to a json file\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_dir = 'RESNET101/MODELS/model_gru.h5'\n",
        "    img_ids, test_references, candidates = generate_captions(model_dir, method='b', beam_width = 20)\n",
        "    res={}\n",
        "    gets={}\n",
        "    for i in range(len(img_ids)):\n",
        "      res[img_ids[i]]=[candidates[i]]\n",
        "      gets[img_ids[i]]=test_references[i]    \n",
        "    import json\n",
        "    with open('res_b.json', 'w') as jsonfile:\n",
        "        json.dump(res, jsonfile)\n",
        "    with open('gets_b.json', 'w') as jsonfile:\n",
        "      json.dump(gets, jsonfile)\n",
        "    print('Captions saved to json file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CG6quOLK376",
        "colab_type": "text"
      },
      "source": [
        "# BLEU (from MSCOCO evaluation server)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGvGwt0FwVLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# bleu_scorer.py\n",
        "# David Chiang <chiang@isi.edu>\n",
        "# Copyright (c) 2004-2006 University of Maryland. All rights\n",
        "# reserved. Do not redistribute without permission from the\n",
        "# author. Not for commercial use.\n",
        "# Modified by: \n",
        "# Hao Fang <hfang@uw.edu>\n",
        "# Tsung-Yi Lin <tl483@cornell.edu>\n",
        "'''Provides:\n",
        "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
        "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
        "'''\n",
        "import copy\n",
        "import sys, math, re\n",
        "from collections import defaultdict\n",
        "\n",
        "def precook_bleu(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs_bleu(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "    reflen = []\n",
        "    maxcounts = {}\n",
        "    for ref in refs:\n",
        "        rl, counts = precook_bleu(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.items():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "    \n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test_bleu(test, crefs, eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "    reflen, refmaxcounts = crefs[0], crefs[1]\n",
        "    testlen, counts = precook_bleu(test, n, True)\n",
        "    result = {}\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "    result[\"testlen\"] = testlen\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.items():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "        \n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs_bleu(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test_bleu(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen        \n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "        \n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self        \n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "        \n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "        \n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:            \n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "                \n",
        "            for key in ['guess','correct']:\n",
        "                for k in range(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in range(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small) \n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in range(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            #if verbose > 1:\n",
        "               # print(comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in range(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in range(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        #if verbose > 0:\n",
        "            #print(totalcomps)\n",
        "            #print(\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOixuqZrwYnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# \n",
        "# File Name : bleu.py\n",
        "#\n",
        "# Description : Wrapper for BLEU scorer.\n",
        "#\n",
        "# Creation Date : 06-01-2015\n",
        "# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n",
        "# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
        "#from bleu_scorer import BleuScorer\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) >= 1)\n",
        "            bleu_scorer += (hypo[0], ref)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ChH904RyAly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleu():\n",
        "    scorer = Bleu(n=4)\n",
        "    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'\n",
        "    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "    print('BLEU-1 = %s' % score[0])\n",
        "    print('BLEU-2 = %s' % score[1])\n",
        "    print('BLEU-3 = %s' % score[2])\n",
        "    print('BLEU-4 = %s' % score[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnYMzX4TurdK",
        "colab_type": "text"
      },
      "source": [
        "# ROUGE-L (from MSCOCO evaluation server)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGfs8-Qmw1AO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# File Name : rouge.py\n",
        "# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n",
        "# Creation Date : 2015-01-07 06:03\n",
        "# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n",
        "\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "def my_lcs(string, sub):\n",
        "    \"\"\"\n",
        "    Calculates longest common subsequence for a pair of tokenized strings\n",
        "    :param string : list of str : tokens from a string split using whitespace\n",
        "    :param sub : list of str : shorter string, also split using whitespace\n",
        "    :returns: length (list of int): length of the longest common subsequence between the two strings\n",
        "\n",
        "    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n",
        "    \"\"\"\n",
        "    if(len(string)< len(sub)):\n",
        "        sub, string = string, sub\n",
        "\n",
        "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
        "\n",
        "    for j in range(1,len(sub)+1):\n",
        "        for i in range(1,len(string)+1):\n",
        "            if(string[i-1] == sub[j-1]):\n",
        "                lengths[i][j] = lengths[i-1][j-1] + 1\n",
        "            else:\n",
        "                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
        "\n",
        "    return lengths[len(string)][len(sub)]\n",
        "\n",
        "class Rouge():\n",
        "    '''\n",
        "    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n",
        "\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        # vrama91: updated the value below based on discussion with Hovey\n",
        "        self.beta = 1.2\n",
        "\n",
        "    def calc_score(self, candidate, refs):\n",
        "        \"\"\"\n",
        "        Compute ROUGE-L score given one candidate and references for an image\n",
        "        :param candidate: str : candidate sentence to be evaluated\n",
        "        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n",
        "        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n",
        "        \"\"\"\n",
        "        # assert(len(candidate)==1)\n",
        "        # assert(len(refs)>0)\n",
        "        prec = []\n",
        "        rec = []\n",
        "\n",
        "        # split into tokens\n",
        "        token_c = candidate[0].split(\" \")\n",
        "    \t\n",
        "        for reference in refs:\n",
        "            # split into tokens\n",
        "            token_r = reference.split(\" \")\n",
        "            # compute the longest common subsequence\n",
        "            lcs = my_lcs(token_r, token_c)\n",
        "            prec.append(lcs/float(len(token_c)))\n",
        "            rec.append(lcs/float(len(token_r)))\n",
        "\n",
        "        prec_max = max(prec)\n",
        "        rec_max = max(rec)\n",
        "\n",
        "        if(prec_max!=0 and rec_max !=0):\n",
        "            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
        "        else:\n",
        "            score = 0.0\n",
        "        return score\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        \"\"\"\n",
        "        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n",
        "        Invoked by evaluate_captions.py \n",
        "        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n",
        "        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n",
        "        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n",
        "        \"\"\"\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        score = []\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref  = gts[id]\n",
        "\n",
        "            score.append(self.calc_score(hypo, ref))\n",
        "\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) > 0)\n",
        "\n",
        "        average_score = np.mean(np.array(score))\n",
        "        return average_score, np.array(score)\n",
        "\n",
        "    def method(self):\n",
        "        return \"Rouge\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYe7pQPpt43C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rouge():\n",
        "    scorer = Rouge()\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "    print('ROUGE-L = %s' % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Okc7_7suhzT",
        "colab_type": "text"
      },
      "source": [
        "# CIDEr-D (from MSCOCO evaluation server)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9j6qslawfu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# Tsung-Yi Lin <tl483@cornell.edu>\n",
        "# Ramakrishna Vedantam <vrama91@vt.edu>\n",
        "\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pdb\n",
        "import math\n",
        "\n",
        "def precook_cider(s, n=4, out=False):\n",
        "    \"\"\"\n",
        "    Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\n",
        "    :param s: string : sentence to be converted into ngrams\n",
        "    :param n: int    : number of ngrams for which representation is calculated\n",
        "    :return: term frequency vector for occuring ngrams\n",
        "    \"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "def cook_refs_cider(refs, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.\n",
        "    :param refs: list of string : reference sentences for some image\n",
        "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
        "    :return: result (list of dict)\n",
        "    '''\n",
        "    return [precook_cider(ref, n) for ref in refs]\n",
        "\n",
        "def cook_test_cider(test, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.\n",
        "    :param test: list of string : hypothesis sentence for some image\n",
        "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
        "    :return: result (dict)\n",
        "    '''\n",
        "    return precook_cider(test, n, True)\n",
        "\n",
        "class CiderScorer(object):\n",
        "    \"\"\"CIDEr scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = CiderScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
        "        ''' singular instance '''\n",
        "        self.n = n\n",
        "        self.sigma = sigma\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.document_frequency = defaultdict(float)\n",
        "        self.cook_append(test, refs)\n",
        "        self.ref_len = None\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs_cider(refs))\n",
        "            if test is not None:\n",
        "                self.ctest.append(cook_test_cider(test)) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new CiderScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "\n",
        "        return self\n",
        "    def compute_doc_freq(self):\n",
        "        '''\n",
        "        Compute term frequency for reference data.\n",
        "        This will be used to compute idf (inverse document frequency later)\n",
        "        The term frequency is stored in the object\n",
        "        :return: None\n",
        "        '''\n",
        "        for refs in self.crefs:\n",
        "            # refs, k ref captions of one image\n",
        "            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n",
        "                self.document_frequency[ngram] += 1\n",
        "            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    def compute_cider(self):\n",
        "        def counts2vec(cnts):\n",
        "            \"\"\"\n",
        "            Function maps counts of ngram to vector of tfidf weights.\n",
        "            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n",
        "            The n-th entry of array denotes length of n-grams.\n",
        "            :param cnts:\n",
        "            :return: vec (array of dict), norm (array of float), length (int)\n",
        "            \"\"\"\n",
        "            vec = [defaultdict(float) for _ in range(self.n)]\n",
        "            length = 0\n",
        "            norm = [0.0 for _ in range(self.n)]\n",
        "            for (ngram, term_freq) in cnts.items():\n",
        "                # give word count 1 if it doesn't appear in reference corpus\n",
        "                df = np.log(max(1.0, self.document_frequency[ngram]))\n",
        "                # ngram index\n",
        "                n = len(ngram)-1\n",
        "                # tf (term_freq) * idf (precomputed idf) for n-grams\n",
        "                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n",
        "                # compute norm for the vector.  the norm will be used for computing similarity\n",
        "                norm[n] += pow(vec[n][ngram], 2)\n",
        "\n",
        "                if n == 1:\n",
        "                    length += term_freq\n",
        "            norm = [np.sqrt(n) for n in norm]\n",
        "            return vec, norm, length\n",
        "\n",
        "        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n",
        "            '''\n",
        "            Compute the cosine similarity of two vectors.\n",
        "            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n",
        "            :param vec_ref: array of dictionary for vector corresponding to reference\n",
        "            :param norm_hyp: array of float for vector corresponding to hypothesis\n",
        "            :param norm_ref: array of float for vector corresponding to reference\n",
        "            :param length_hyp: int containing length of hypothesis\n",
        "            :param length_ref: int containing length of reference\n",
        "            :return: array of score for each n-grams cosine similarity\n",
        "            '''\n",
        "            delta = float(length_hyp - length_ref)\n",
        "            # measure consine similarity\n",
        "            val = np.array([0.0 for _ in range(self.n)])\n",
        "            for n in range(self.n):\n",
        "                # ngram\n",
        "                for (ngram,count) in vec_hyp[n].items():\n",
        "                    # vrama91 : added clipping\n",
        "                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n",
        "\n",
        "                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n",
        "                    val[n] /= (norm_hyp[n]*norm_ref[n])\n",
        "\n",
        "                assert(not math.isnan(val[n]))\n",
        "                # vrama91: added a length based gaussian penalty\n",
        "                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n",
        "            return val\n",
        "\n",
        "        # compute log reference length\n",
        "        self.ref_len = np.log(float(len(self.crefs)))\n",
        "        if len(self.crefs) == 1:\n",
        "            self.ref_len = 1\n",
        "        scores = []\n",
        "        for test, refs in zip(self.ctest, self.crefs):\n",
        "            # compute vector for test captions\n",
        "            vec, norm, length = counts2vec(test)\n",
        "            # compute vector for ref captions\n",
        "            score = np.array([0.0 for _ in range(self.n)])\n",
        "            for ref in refs:\n",
        "                vec_ref, norm_ref, length_ref = counts2vec(ref)\n",
        "                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n",
        "            # change by vrama91 - mean of ngram scores, instead of sum\n",
        "            score_avg = np.mean(score)\n",
        "            # divide by number of references\n",
        "            score_avg /= len(refs)\n",
        "            # multiply score by 10\n",
        "            score_avg *= 10.0\n",
        "            # append score of an image to the score list\n",
        "            scores.append(score_avg)\n",
        "        return scores\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        # compute idf\n",
        "        self.compute_doc_freq()\n",
        "        # assert to check document frequency\n",
        "        assert(len(self.ctest) >= max(self.document_frequency.values()))\n",
        "        # compute cider score\n",
        "        score = self.compute_cider()\n",
        "        # debug\n",
        "        # print score\n",
        "        return np.mean(np.array(score)), np.array(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PogLBmpwiF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filename: cider.py\n",
        "# Description: Describes the class to compute the CIDEr (Consensus-Based Image Description Evaluation) Metric \n",
        "#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n",
        "# Creation Date: Sun Feb  8 14:16:54 2015\n",
        "# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n",
        "#from cider_scorer import CiderScorer\n",
        "import pdb\n",
        "\n",
        "class Cider:\n",
        "    \"\"\"\n",
        "    Main Class to compute the CIDEr metric\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
        "        # set cider to sum over 1 to 4-grams\n",
        "        self._n = n\n",
        "        # set the standard deviation parameter for gaussian penalty\n",
        "        self._sigma = sigma\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "        \"\"\"\n",
        "        Main function to compute CIDEr score\n",
        "        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n",
        "                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n",
        "        :return: cider (float) : computed CIDEr score for the corpus\n",
        "        \"\"\"\n",
        "        assert(gts.keys() == res.keys())\n",
        "        imgIds = gts.keys()\n",
        "\n",
        "        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n",
        "        for id in imgIds:\n",
        "            hypo = res[id]\n",
        "            ref = gts[id]\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is list)\n",
        "            assert(len(hypo) == 1)\n",
        "            assert(type(ref) is list)\n",
        "            assert(len(ref) > 0)\n",
        "            cider_scorer += (hypo[0], ref)\n",
        "        (score, scores) = cider_scorer.compute_score()\n",
        "\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"CIDEr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWn67e-_td_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cider():\n",
        "    scorer = Cider()\n",
        "    (score, scores) = scorer.compute_score(gts, res)\n",
        "    print('CIDER-D = %s' % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZXw2RMLayHa",
        "colab_type": "text"
      },
      "source": [
        "#SCORES FOR CAPTIONS GENERATED BY GREEDY SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdhJvLK4tQ9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('/content/gets_g.json', 'r') as file:\n",
        "    gts = json.load(file)\n",
        "with open('/content/res_g.json', 'r') as file:\n",
        "    res = json.load(file)\n",
        "print('Scores for captions generated by greedy search')\n",
        "bleu()\n",
        "rouge()\n",
        "cider()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbp0Ovw8a7O0",
        "colab_type": "text"
      },
      "source": [
        "#SCORES FOR CAPTIONS GENERATED BY BEAM SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd09ULGjNjJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('/content/gets_b.json', 'r') as file:\n",
        "    gts = json.load(file)\n",
        "with open('/content/res_b.json', 'r') as file:\n",
        "    res = json.load(file)\n",
        "print('Scores for captions generated by beam search')\n",
        "bleu()\n",
        "rouge()\n",
        "cider()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dEUn1DULNrG9"
      },
      "source": [
        "# GENERATE CAPTIONS FOR NEW IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWm0fw_0NrG7",
        "colab": {}
      },
      "source": [
        "train_dir = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "token_dir = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "model_dir = 'RESNET101/MODELS/model_gru.h5'\n",
        "tokenizer = create_tokenizer(train_dir, token_dir, start_end = True, use_all=True)\n",
        "vocab_size  = tokenizer.num_words or (len(tokenizer.word_index)+1)\n",
        "max_len = 24 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bQQ_vFiHNrG2"
      },
      "source": [
        "# Greedy search decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6yLvOHaANrGx",
        "colab": {}
      },
      "source": [
        "# generate captions for test images using greedy search\n",
        "# the images are to be kept in a directory\n",
        "\n",
        "par_inference = greedy_model(vocab_size, max_len)\n",
        "par_inference.load_weights(model_dir, by_name = True, skip_mismatch=True)\n",
        "\n",
        "def generate_caption_from_directory(file_directory):\n",
        "    # Encoder\n",
        "    img_features_dict = feature_extraction(file_directory)\n",
        "    # Decoder\n",
        "    captions = decoder_greedy(par_inference, tokenizer, img_features_dict['features'], True)    \n",
        "    return img_features_dict['ids'], captions\n",
        "\n",
        "image_dir = 'test_images' #folder containing the test images\n",
        "img_names, captions = generate_caption_from_directory(image_dir)\n",
        "for img_file in os.listdir(image_dir):\n",
        "    img = mpimg.imread(image_dir + '/' + img_file)\n",
        "    plt.imshow(img)    \n",
        "    img_name = os.path.splitext(img_file)[0]\n",
        "    idx = img_names.index(img_name)    \n",
        "    plt.show()\n",
        "    print(captions[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ydO2uyb_NrGr"
      },
      "source": [
        "# Beam search decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Gd_GclPNrGY",
        "colab": {}
      },
      "source": [
        "# generate captions for test images using beam search\n",
        "# the images are to be kept in a directory\n",
        "\n",
        "beamsearching_model = beamsearch_model(vocab_size)\n",
        "beamsearching_model.load_weights(model_dir, by_name = True, skip_mismatch=True)\n",
        "\n",
        "def generate_caption_from_directory(file_directory, beam_width = 20):\n",
        "    # Encoder\n",
        "    img_features_dict = feature_extraction(file_directory)\n",
        "    # Decoder\n",
        "    N = img_features_dict['features'].shape[0]\n",
        "    a0= np.zeros([N, 512])\n",
        "    captions = []\n",
        "    for i in range(N):\n",
        "        res = beam_searching(beamsearching_model, img_features_dict['features'][i, :].reshape(1,-1), a0[i, :].reshape(1,-1), tokenizer, beam_width, max_len)\n",
        "        best_idx = np.argmax(res['scores'])\n",
        "        captions.append(tokenizer.sequences_to_texts([res['routes'][best_idx]])[0])   \n",
        "    return img_features_dict['ids'], captions\n",
        "\n",
        "image_dir = 'test_images' #folder containing test images\n",
        "img_names, captions = generate_caption_from_directory(image_dir, 20)\n",
        "for img_file in os.listdir(image_dir):\n",
        "    img = mpimg.imread(image_dir + '/' + img_file)\n",
        "    plt.imshow(img)   \n",
        "    img_name = os.path.splitext(img_file)[0]\n",
        "    idx = img_names.index(img_name)    \n",
        "    plt.show()\n",
        "    print(captions[idx])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}